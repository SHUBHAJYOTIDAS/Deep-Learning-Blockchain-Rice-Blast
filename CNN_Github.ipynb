{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4097709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten, Add,Concatenate,concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import  GlobalAveragePooling2D,Dropout,Conv2D,MaxPool2D,UpSampling2D,LeakyReLU,AveragePooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import keras.backend as K\n",
    "from keras.layers import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4adfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_load= np.load(\"DATA/B_train.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fd9a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_load =np.load(\"DATA/B_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c453cb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train=np.load(\"DATA/Y_train.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36744e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test=np.load(\"DATA/Y_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91147888",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=[]\n",
    "for i in range(len(Y_train)):\n",
    "    Y.append(int(Y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab391f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1=[]\n",
    "for i in range(len(Y_test)):\n",
    "    Y1.append(int(Y_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5678643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_LABEL=[]\n",
    "for j in range(len(Y)):\n",
    "    if Y[j]==1:\n",
    "        FINAL_LABEL.append([1,0,0,0])\n",
    "    elif Y[j]==2:\n",
    "        FINAL_LABEL.append([0,1,0,0])\n",
    "    elif Y[j]==3:\n",
    "        FINAL_LABEL.append([0,0,1,0])\n",
    "    else:\n",
    "        FINAL_LABEL.append([0,0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59066e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_LABEL_test=[]\n",
    "for j in range(len(Y1)):\n",
    "    if Y1[j]==1:\n",
    "        FINAL_LABEL_test.append([1,0,0,0])\n",
    "    elif Y1[j]==2:\n",
    "        FINAL_LABEL_test.append([0,1,0,0])\n",
    "    elif Y1[j]==3:\n",
    "        FINAL_LABEL_test.append([0,0,1,0])\n",
    "    else:\n",
    "        FINAL_LABEL_test.append([0,0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40df5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1=np.array(FINAL_LABEL)\n",
    "y2=np.array(FINAL_LABEL_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b33413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "class attention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(attention,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.W1=self.add_weight(name='attention_weight1', shape=(128,1),\n",
    "                               initializer='random_normal', trainable=True)\n",
    "        self.b1=self.add_weight(name='attention_bias1', shape=(1,),\n",
    "                               initializer='zeros', trainable=True)\n",
    "        self.W2=self.add_weight(name='attention_weight2', shape=(128,1),\n",
    "                               initializer='random_normal', trainable=True)\n",
    "        self.b2=self.add_weight(name='attention_bias2', shape=(1,),\n",
    "                               initializer='zeros', trainable=True)\n",
    "        self.W3=self.add_weight(name='attention_weight3', shape=(128,1),\n",
    "                               initializer='random_normal', trainable=True)\n",
    "        self.b3=self.add_weight(name='attention_bias3', shape=(1,),\n",
    "                               initializer='zeros', trainable=True)\n",
    "\n",
    "        self.W4=self.add_weight(name='attention_weight4', shape=(3,1),\n",
    "                               initializer='random_normal', trainable=True)\n",
    "        self.b4=self.add_weight(name='attention_bias4', shape=(1,),\n",
    "                               initializer='zeros', trainable=True)\n",
    "        super(attention, self).build(input_shape)\n",
    "\n",
    "    def call(self,x):\n",
    "        # Alignment scores. Pass them through tanh function\n",
    "        e1 = K.relu(K.dot(x,self.W1)+self.b1)\n",
    "        e2 = K.relu(K.dot(x,self.W2)+self.b2)\n",
    "        e3 = K.relu(K.dot(x,self.W3)+self.b3)\n",
    "        e_head=tf.concat([e1, e2,e3],-1)\n",
    "        e = K.relu(K.dot(e_head,self.W4)+self.b4)\n",
    "        alpha = K.softmax(e)\n",
    "        # Reshape to tensorFlow format\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        # Compute the context vector\n",
    "        context = x * alpha\n",
    "        context = K.sum(context, axis=1)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3614853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN(input_shape):\n",
    "    X_input = Input(input_shape)\n",
    "    conv1=Conv2D(32, kernel_size=(3, 3),padding=\"same\",strides=(1,1))(X_input)\n",
    "    conv1=BatchNormalization()(conv1)\n",
    "    conv1=LeakyReLU(alpha=0.1)(conv1)\n",
    "\n",
    "\n",
    "\n",
    "    ## downsampling\n",
    "    conv2=Conv2D(64, kernel_size=(3, 3),strides=(2,2))(conv1)\n",
    "    conv2=BatchNormalization()(conv2)\n",
    "    conv2=LeakyReLU(alpha=0.1)(conv2)\n",
    "\n",
    "\n",
    "\n",
    "    ## 1st block\n",
    "    conv3=Conv2D(32, kernel_size=(1, 1),\n",
    "                     strides=(1,1),padding=\"same\")(conv2)\n",
    "    conv3=BatchNormalization()(conv3)\n",
    "    conv3=LeakyReLU(alpha=0.1)(conv3)\n",
    "\n",
    "\n",
    "    conv4=Conv2D(64, kernel_size=(3, 3),\n",
    "                     strides=(1,1),padding=\"same\")(conv3)\n",
    "    conv4=BatchNormalization()(conv4)\n",
    "    conv4=LeakyReLU(alpha=0.1)(conv4)\n",
    "\n",
    "    conv5=Add()([conv2,conv4])\n",
    "\n",
    "\n",
    "    ## DOWNSAMPLING\n",
    "    conv6=Conv2D(128, kernel_size=(3, 3),strides=(2,2))(conv5)\n",
    "    conv6=BatchNormalization()(conv6)\n",
    "    conv6=LeakyReLU(alpha=0.1)(conv6)\n",
    "\n",
    "    ## 2nd block\n",
    "    conv7=Conv2D(64, kernel_size=(1, 1),strides=(1,1),padding=\"same\")(conv6)\n",
    "    conv7=BatchNormalization()(conv7)\n",
    "    conv7=LeakyReLU(alpha=0.1)(conv7)\n",
    "\n",
    "    conv8=Conv2D(128, kernel_size=(3, 3),strides=(1,1),padding=\"same\")(conv7)\n",
    "    conv8=BatchNormalization()(conv8)\n",
    "    conv8=LeakyReLU(alpha=0.1)(conv8)\n",
    "\n",
    "    conv9=Add()([conv6,conv8])\n",
    "\n",
    "    conv10=Conv2D(64, kernel_size=(1, 1),strides=(1,1),padding=\"same\")(conv9)\n",
    "\n",
    "\n",
    "    ## downsampling\n",
    "    conv13=Conv2D(512, kernel_size=(3, 3),\n",
    "                    strides=(2,2))(conv10)\n",
    "    conv13=BatchNormalization()(conv13)\n",
    "    conv13=LeakyReLU(alpha=0.1)(conv13)\n",
    "\n",
    "    ## 3rd block\n",
    "    conv14=Conv2D(256, kernel_size=(1, 1),\n",
    "                    strides=(1,1),padding=\"same\")(conv13)\n",
    "    conv14=BatchNormalization()(conv14)\n",
    "    conv14=LeakyReLU(alpha=0.1)(conv14)\n",
    "\n",
    "    conv15=Conv2D(512, kernel_size=(3, 3),\n",
    "                    strides=(1,1),padding=\"same\")(conv14)\n",
    "    conv15=BatchNormalization()(conv15)\n",
    "    conv15=LeakyReLU(alpha=0.1)(conv15)\n",
    "\n",
    "    conv16=Add()([conv13,conv15])\n",
    "\n",
    "    conv17=Conv2D(256, kernel_size=(1, 1),\n",
    "                    strides=(1,1),padding=\"same\")(conv16)\n",
    "    conv17=BatchNormalization()(conv17)\n",
    "    conv17=LeakyReLU(alpha=0.1)(conv17)\n",
    "\n",
    "    conv18=Conv2D(512, kernel_size=(3, 3),\n",
    "                    strides=(1,1),padding=\"same\")(conv17)\n",
    "    conv18=BatchNormalization()(conv18)\n",
    "    conv18=LeakyReLU(alpha=0.1)(conv18)\n",
    "\n",
    "    conv19=Add()([conv16,conv18])\n",
    "\n",
    "#     ## downsampling\n",
    "    conv26=Conv2D(1024, kernel_size=(3, 3),\n",
    "                    strides=(2,2))(conv19)\n",
    "    conv26=BatchNormalization()(conv26)\n",
    "    conv26=LeakyReLU(alpha=0.1)(conv26)\n",
    "    ## 4th block\n",
    "    conv27=Conv2D(512, kernel_size=(1, 1),\n",
    "                    strides=(1,1),padding=\"same\")(conv26)\n",
    "    conv27=BatchNormalization()(conv27)\n",
    "    conv27=LeakyReLU(alpha=0.1)(conv27)\n",
    "\n",
    "    conv28=Conv2D(1024, kernel_size=(3, 3),\n",
    "                    strides=(1,1),padding=\"same\")(conv27)\n",
    "    conv28=BatchNormalization()(conv28)\n",
    "    conv28=LeakyReLU(alpha=0.1)(conv28)\n",
    "\n",
    "    conv29=Add()([conv26,conv28])\n",
    "\n",
    "    conv30=Conv2D(512, kernel_size=(1, 1),\n",
    "                    strides=(1,1),padding=\"same\")(conv29)\n",
    "    conv30=BatchNormalization()(conv30)\n",
    "    conv30=LeakyReLU(alpha=0.1)(conv30)\n",
    "\n",
    "    conv31=Conv2D(1024, kernel_size=(3, 3),\n",
    "                    strides=(1,1),padding=\"same\")(conv30)\n",
    "    conv31=BatchNormalization()(conv31)\n",
    "    conv31=LeakyReLU(alpha=0.1)(conv31)\n",
    "    conv32=Add()([conv29,conv31])\n",
    "    #     ## downsampling\n",
    "    conv33=Conv2D(1024, kernel_size=(3, 3),\n",
    "                    strides=(2,2))(conv32)\n",
    "    conv33=BatchNormalization()(conv33)\n",
    "    conv33=LeakyReLU(alpha=0.1)(conv33)\n",
    "    ## 4th block\n",
    "    conv34=Conv2D(512, kernel_size=(1, 1),\n",
    "                    strides=(1,1),padding=\"same\")(conv33)\n",
    "    conv34=BatchNormalization()(conv34)\n",
    "    conv34=LeakyReLU(alpha=0.1)(conv34)\n",
    "\n",
    "    conv35=Conv2D(1024, kernel_size=(3, 3),\n",
    "                    strides=(1,1),padding=\"same\")(conv34)\n",
    "    conv35=BatchNormalization()(conv35)\n",
    "    conv35=LeakyReLU(alpha=0.1)(conv35)\n",
    "\n",
    "    conv36=Add()([conv33,conv35])\n",
    "\n",
    "    conv37=Conv2D(512, kernel_size=(1, 1),\n",
    "                    strides=(1,1),padding=\"same\")(conv36)\n",
    "    conv37=BatchNormalization()(conv37)\n",
    "    conv37=LeakyReLU(alpha=0.1)(conv37)\n",
    "\n",
    "    conv38=Conv2D(1024, kernel_size=(3, 3),\n",
    "                    strides=(1,1),padding=\"same\")(conv37)\n",
    "    conv38=BatchNormalization()(conv38)\n",
    "    conv38=LeakyReLU(alpha=0.1)(conv38)\n",
    "    conv39=Add()([conv36,conv38])\n",
    "    conv100=Flatten()(conv39)\n",
    "    conv105=Dense(128, activation='ReLU')(conv100)\n",
    "    conv106=Dense(64, activation='ReLU')(conv105)\n",
    "    conv107=Dense(32, activation='ReLU')(conv106)\n",
    "    conv108=Dense(16, activation='ReLU')(conv107)\n",
    "    prediction = Dense(4, activation='softmax')(conv108)\n",
    "    model=Model(inputs=X_input, outputs=prediction)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a55241",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape= (224,224,1)\n",
    "cnn = CNN(input_shape)\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7d4eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52d3429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "mc = ModelCheckpoint('1model_B_high.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624511f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = cnn.fit(\n",
    "  train_load,y1,\n",
    "  validation_data=(test_load,y2),\n",
    "  epochs=50,batch_size=32,\n",
    "  callbacks=[mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da7cbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.save(\"/content/drive/MyDrive/DATA/1model_B_high.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b038cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = cnn\n",
    "# Choose a specific layer to extract features from\n",
    "desired_layer = base_model.get_layer('flatten')\n",
    "feature_extractor_model = Model(inputs=base_model.input, outputs=desired_layer.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22486a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "F=[]\n",
    "for i in tqdm(range(len(train_load))):\n",
    "    image=train_load[i].reshape([-1,224,224,1])\n",
    "    features=feature_extractor_model.predict(image)\n",
    "    F.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd37ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"/content/drive/MyDrive/DATA/B_features_train_CNN.npy\",F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b2335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_load= np.load(\"DATA/B_features_train_CNN.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd0930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_load =np.load(\"B_features_test_CNN.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7615401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attentions(input_shape):\n",
    "    X_input = Input(input_shape)\n",
    "    A1 = attention(input_shape=input_shape)(X_input)\n",
    "    prediction = Dense(4, activation='softmax')(A1)\n",
    "    model=Model(inputs=X_input, outputs=prediction)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad6f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=(128,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b537ee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "att = attentions(input_shape)\n",
    "att.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cbb175",
   "metadata": {},
   "outputs": [],
   "source": [
    "att.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52720186",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = att.fit(\n",
    "  train,y1,\n",
    "  validation_data=(test,y2),\n",
    "  epochs=500,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b0f0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = att\n",
    "# Choose a specific layer to extract features from\n",
    "desired_layer = base_model.get_layer('attention')\n",
    "feature_extractor_model = Model(inputs=base_model.input, outputs=desired_layer.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b61f4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Features=[]\n",
    "for i in tqdm(range(len(train))):\n",
    "    x=train[i].reshape([-1,128])\n",
    "    m=feature_extractor_model.predict(x)\n",
    "    Features.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c587513",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"/content/drive/MyDrive/DATA/R_features_train_CNN_attention.npy\",Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b4a62a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74eeee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e8b14d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0935321b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
